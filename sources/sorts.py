# coding: utf-8

"""
__Оценка сложности алгоритмов, сортировки__

Материалы: лекция 1 (13.01), 2 (16.05), семинар 2 (15.01).

Лектор: Объедков Сергей Александрович<br>
Семинарист: Макаров Илья Андреевич<br>
Конспектировал Гончаров Владимир

"""

import random
import speedtest

"""
## Оценка сложности алгоритмов

Для оценки времени и памяти, которая уходит на выполнение алгоритма на
входных данных размера _n_ нам нужно знать, сколько операций совершит алгоритм
и сколько памяти он потребляет. Однако часто это сделать сложно,
особенно если мы ничего не знаем о природе данных (как долго происходит
сложение, к примеру?) и архитектуре.

Но часто это и не требуется: мы лишь хотим оценить, как растет время
(или память) с ростом входных данных. Для этого можно математическими
методами оценить количество требуемых ресурсов и записать ответ в виде
эквивалентных функций.

Применяются следующие обозначения:

\\(f = \bar{\bar{o}}(g) \Rightarrow \forall ~ c > 0 ~ \exists ~ n_0 > 0 :
    \forall ~ n > n_0 \Rightarrow (0 \le f < cg)(n) \\)

\\(f = \omega(g) \Rightarrow \forall ~ c > 0 ~ \exists ~ n_0 > 0 :
    \forall ~ n > n_0 \Rightarrow (0 \le cf < g)(n) \\)

\\(f = O(g) \Rightarrow \exists ~ c, n_0 > 0 :
    \forall ~ n > n_0 \Rightarrow (0 \le f \le cg)(n) \\)

\\(f = \Omega(g) \Rightarrow \exists ~ c, n_0 > 0 :
    \forall ~ n > n_0 \Rightarrow (0 \le cf \le g)(n) \\)

\\(f = \Theta(g) \Rightarrow \exists ~ c_1, c_2, n_0 > 0 :
    \forall ~ n > n_0 \Rightarrow (0 \le c_1g \le f \le c_2g)(n)\\)

Более подробное описание терминов [на википедии][1].

Стоит заметить, что _«О»_ задают класс функций.

[1]:https://ru.wikipedia.org/wiki/«O»_большое_и_«o»_малое#.D0.94.D1.80.D1.83.D0.B3.D0.B8.D0.B5_.D0.BF.D0.BE.D0.B4.D0.BE.D0.B1.D0.BD.D1.8B.D0.B5_.D0.BE.D0.B1.D0.BE.D0.B7.D0.BD.D0.B0.D1.87.D0.B5.D0.BD.D0.B8.D1.8F

"""

#d

"""
## Сортировка выбором (selection sort)

В представлении не нуждается, работает за \\(O(n^2) \\), есть стабильный вариант.

"""
def selection_sort(a):
    # Нестабильный вариант
    # Обратите внимание, что эта функция является
    # модификативной (in place) и использует O(1) дополнительной памяти.
    i = len(a)
    while i > 1:
        max = 0
        for j in range(i):
            if a[j] > a[max]:
                max = j
        a[i - 1], a[max] = a[max], a[i - 1]
        i -= 1
    return a

#d

"""
## Сортировка вставками (insertion sort)

Устойчивая сортировка, эффективная на небольших объемах данных.

На каждом шаге элемент перемещается назад по массиву до тех пор, пока
перед ним находится больший элемент. Таким образом у нас есть
начало массива, отсортированное на предыдущих шагах и элемент
на _i_-м месте, который нужно поместить в отсортированную часть так,
чтобы она осталась отсортированной.

«Данный алгоритм можно ускорить при помощи использования
бинарного поиска для нахождения места текущему элементу в
отсортированной части. Проблема с долгим сдвигом массива вправо
решается при помощи смены указателей»&nbsp;— Википедия

__Время работы:__

\\(O(n^2) \\) в худшем случае: входной массив отсортирован в порядке,
обратном нужному, на каждом участке цикла по _i_ происходит
_i_ операций в цикле по _k_.

\\(O(n) \\) в лучшем: массив уже отсортирован,
на каждом участке цикла по _i_ происходит \\( O(1)\\) операций.

"""
def insertion_sort(a, left=0, right=None):
    if right is None:
        right = len(a)
    # Как и `selection_sort`, in place функция
    # использует O(1) дополнительной памяти.
    for i in range(left + 1, right):
        key = a[i]
        for j in range(i - 1, left - 1, -1):
            if key < a[j]:
                a[j], a[j + 1] = a[j + 1], a[j]
            else:
                break
    return a

#d

"""
## Сортировка слиянием (merge sort)

Алгоритм рекурсивный: на каждом шаге мы разделяем массив
на две части и сортируем каждую отдельно (тем же самым алгоритмом),
после чего сливаем две части вместе.

__Время работы__ \\(O(n \log n) \\).

Если время работы алгоритма на массиве длинны \\(n \\)&nbsp;— \\(T[n] \\),
то по определению справедливо рекуррентное соотношение
\\(T[n] = 2T[n/2] + O(n) \\): две сортировки массивов размера
\\(n/2 \\) и еще n операций на слияние.

Можно убедиться, что \\(T[n] = O(n \log n) \\), используя
[основную теорему о рекуррентных соотношениях][1].

[1]: https://ru.wikipedia.org/wiki/Основная_теорема_о_рекуррентных_соотношениях

"""
def merge_sort(a):
    result = []
    if len(a) <= 20:
        # Часто insertion_sort показывает лучшие результаты
        # на небольших массивах.
        return insertion_sort(a)
    else:
        x = merge_sort(a[:len(a) // 2])
        y = merge_sort(a[len(a) // 2:])

        i, j = 0, 0
        while i < len(x) and j < len(y):
            if x[i] > y[j]:
                result.append(y[j])
                j += 1
            else:
                result.append(x[i])
                i += 1
        result += x[i:]
        result += y[j:]
        return result

#d

"""
## Быстрая сортировка (qsort)

Алгоритм быстрой сортировки&nbsp;— объединение идеи пузырьковой сортировки
и сортировки слиянием.

На каждом шаге мы выбираем «опорный элемент» («pivot element»),
после чего переносим все элементы, меньшие опорного,&nbsp;— налево,
большие&nbsp;— направо. После этого мы запускаем сортировку на левой
и правой части массива (центральную не трогаем). Заметим, что после
этой операции не нужно проводить слияние.

__Опорный элемент:__

Выбор опорного элемента напрямую влияет на скорость алгоритма:
самым оптимальным выбором будет медиана, самым худшим&nbsp;—
выбор наименьшего или наибольшего элемента. Так как искать медиану
без дополнительных элементов сложно, используют разные ухищрения:
средний из максимального и минимального элемента, случайный элемент,
медиана из первого, среднего и последнего элемента и т. д.

__Время работы:__

В in place реализации выполняется в среднем за \\(O(n \log n) \\).

При этом в худшем случае, когда в качестве опорного элемента
выбирается минимальный или максимальный элемент,
на каждом шаге мы имеем деление массива
на отрезки [0] и [1, n) за \\(n - 1 \\) операцию обмена,
что ухудшает время до \\(O(n^2) \\).

Средний случай подробнее:

Средний, то есть «удачный» случай разделения — случай, когда
опорный элемент близок к медиане. Если предположить, что он
попадает в центральную чать массива (т. е. отстоит от медианы не более,
чем на _0,25n_, вероятность чего составит 50% при случайном выборе элемента),
глубина рекурсии составит не более, чем \\(\log_{4/3} n\\), и поскольку
на каждом уровне рекурсии выполняется _n_ операций, мы получаем оценку в
\\(O(n \log n) \\).

__Глубина рекурсии:__

В худшем случае глубина рекурсии составляет \\(O(n) \\).
Существует несколько выходов из ситуации. В их числе:

*   Переход на другой вид сортировки при привышении лимита глубины
    (см. [introsort](https://ru.wikipedia.org/wiki/Introsort))
*   Перестройка алгоритма с использованием хвостовой рекурсии:
    сортировать две полученные части массива в рамках одного вызова.
    К примеру, вместо `left` и `right` передавать список вида
    `[[left, middle], [middle, right]]` и сортировать отрезки в одном цикле.

"""
def quick_sort_inplace(a, left=0, right=None):
    if right is None:
        right = len(a)

    if left < right - 1:
        i, j = left, right - 1

        # В качестве опорного элемента выбирается случайный.
        pivot = random.choice(a[i:j])

        while i <= j:
            while a[i] < pivot:
                i += 1
            while a[j] > pivot:
                j -= 1
            if i <= j:
                a[i], a[j] = a[j], a[i]
                i += 1
                j -= 1

        quick_sort_inplace(a, left, j + 1)
        quick_sort_inplace(a, i, right)

    return a


def quick_sort(a):
    # Пример реализации с использованием списковых сборок.
    # Красиво, компактно, но памяти используется больше.
    if len(a) <= 1:
        return a
    else:
        pivot = random.choice(a)
        return (quick_sort([x for x in a if x < pivot]) +
                [pivot] * a.count(pivot) +
                quick_sort([x for x in a if x > pivot]))

#d

"""
## Сортировка кучей (heap sort, пирамидальная сортировка)

Описание скоро будет...

"""
def heap_sort(a):
    # FIXME
    # Тесты показывают, что этот алгоритм ужасен.
    # Возможно, у кого-то есть более быстрая реализация — пришлите, пожалуйста

    def l_ch(i):
        # Left child
        return i * 2 + 1

    def r_ch(i):
        # Right child
        return i * 2 + 2

    def push_down(i, max_depth):
        # `max_depth` нужна, чтобы не трогать отсортированную часть кучи

        get_gt = lambda i, j: i if a[i] > a[j] else j

        while l_ch(i) < max_depth:
            # Выбираем элемент, на место которого нужно протолкнуть корень
            gt_ch = (get_gt(l_ch(i), r_ch(i))
                     if r_ch(i) < max_depth else
                     l_ch(i))

            if a[i] < a[gt_ch]:
                a[i], a[gt_ch] = a[gt_ch], a[i]
            else:
                break

            i = gt_ch

    # Строим кучу
    for i in range((len(a) // 2) - 1, -1, -1):
        push_down(i, len(a))

    # Сортируем кучу
    for i in range(len(a)-1, 0, -1):
        a[i], a[0] = a[0], a[i]
        push_down(0, i)

    return a

